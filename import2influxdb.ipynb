{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1130b39-c7ba-4a82-b95d-c1954cb3f1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from tqdm.notebook import tqdm  # Changed from tqdm.notebook\n",
    "import logging\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# InfluxDB configuration\n",
    "bucket = \"crypto\"\n",
    "org = os.environ[\"INFLUXDB_ORG\"]\n",
    "token = os.environ[\"INFLUXDB_TOKEN\"]\n",
    "url = os.environ[\"INFLUXDB_URL\"]\n",
    "\n",
    "# Optimized client configuration\n",
    "client = InfluxDBClient(\n",
    "    url=url, \n",
    "    token=token, \n",
    "    org=org,\n",
    "    timeout=30000,  # 30 second timeout\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "# Use synchronous write API with batching\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "column_names = [\n",
    "    \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "    \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "]\n",
    "\n",
    "# Columns to convert to numeric (excluding time columns and ignore)\n",
    "numeric_columns = [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\", \"quote_asset_volume\",\n",
    "    \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"\n",
    "]\n",
    "\n",
    "def create_points_batch(df: pd.DataFrame, symbol: str) -> List[Point]:\n",
    "    \"\"\"Create a batch of InfluxDB points from DataFrame rows.\"\"\"\n",
    "    points = []\n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        try:\n",
    "            point = (\n",
    "                Point(\"klines\")\n",
    "                .tag(\"symbol\", symbol)\n",
    "                .field(\"open\", float(row.open))\n",
    "                .field(\"high\", float(row.high))\n",
    "                .field(\"low\", float(row.low))\n",
    "                .field(\"close\", float(row.close))\n",
    "                .field(\"volume\", float(row.volume))\n",
    "                .field(\"quote_asset_volume\", float(row.quote_asset_volume))\n",
    "                .field(\"number_of_trades\", int(row.number_of_trades))\n",
    "                .field(\"taker_buy_base_asset_volume\", float(row.taker_buy_base_asset_volume))\n",
    "                .field(\"taker_buy_quote_asset_volume\", float(row.taker_buy_quote_asset_volume))\n",
    "                .time(int(row.open_time), WritePrecision.MS)\n",
    "            )\n",
    "            points.append(point)\n",
    "        except (ValueError, TypeError, AttributeError) as e:\n",
    "            logger.warning(f\"Skipping row due to data error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return points\n",
    "\n",
    "def process_csv_file(fpath: str, symbol: str, batch_size: int = 5000) -> int:\n",
    "    \"\"\"Process a single CSV file and write to InfluxDB in batches.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing file: {fpath}\")\n",
    "        \n",
    "        # First, peek at the first line to check for headers\n",
    "        with open(fpath, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "        \n",
    "        # Check if first line contains headers (non-numeric values in timestamp position)\n",
    "        skip_header = False\n",
    "        if first_line:\n",
    "            first_value = first_line.split(',')[0].strip()\n",
    "            if not first_value.isdigit():\n",
    "                skip_header = True\n",
    "                logger.info(f\"Detected header row in {fpath}, skipping first row\")\n",
    "        \n",
    "        # Read CSV with optimized settings\n",
    "        df = pd.read_csv(\n",
    "            fpath, \n",
    "            names=column_names, \n",
    "            header=None,\n",
    "            skiprows=1 if skip_header else 0,  # Skip header if detected\n",
    "            dtype={\n",
    "                'open_time': 'str',  # Read as string first for validation\n",
    "                'close_time': 'str',  # Also handle close_time\n",
    "                'number_of_trades': 'Int64'  # Handle potential NaN values\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Read {len(df)} rows from {fpath}\")\n",
    "        \n",
    "        # Filter valid timestamps more efficiently - handle both string and numeric\n",
    "        if len(df) > 0:\n",
    "            # Convert to string if not already, then filter\n",
    "            df['open_time_str'] = df['open_time'].astype(str)\n",
    "            df = df[df[\"open_time_str\"].str.isdigit()].copy()\n",
    "            df = df.drop(columns=['open_time_str'])  # Clean up temp column\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"No valid data in {fpath}\")\n",
    "            return 0\n",
    "        \n",
    "        # Drop ignore column if it exists\n",
    "        if \"ignore\" in df.columns:\n",
    "            df = df.drop(columns=[\"ignore\"])\n",
    "        \n",
    "        # Convert numeric columns efficiently with better error handling\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                # First attempt standard conversion\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                # Check for any remaining non-numeric values that might be headers\n",
    "                non_numeric_mask = df[col].isna() & df[col].notna()\n",
    "                if non_numeric_mask.any():\n",
    "                    logger.warning(f\"Found non-numeric values in column {col}, they will be dropped\")\n",
    "        \n",
    "        # Convert open_time to int64 with better error handling\n",
    "        df['open_time'] = pd.to_numeric(df['open_time'], errors='coerce')\n",
    "        \n",
    "        # Drop rows where open_time conversion failed (likely header rows that slipped through)\n",
    "        df = df[df['open_time'].notna()].copy()\n",
    "        df['open_time'] = df['open_time'].astype('int64')\n",
    "        \n",
    "        # Drop rows with any NaN values in critical columns\n",
    "        initial_rows = len(df)\n",
    "        critical_columns = ['open_time'] + numeric_columns\n",
    "        df = df.dropna(subset=[col for col in critical_columns if col in df.columns])\n",
    "        \n",
    "        if len(df) < initial_rows:\n",
    "            logger.info(f\"Dropped {initial_rows - len(df)} rows with invalid data in {fpath}\")\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"No valid rows remaining after cleaning in {fpath}\")\n",
    "            return 0\n",
    "        \n",
    "        # Process in batches with retry logic\n",
    "        total_rows = len(df)\n",
    "        rows_written = 0\n",
    "        max_retries = 3\n",
    "        \n",
    "        # Create progress bar for batches\n",
    "        batch_count = (total_rows + batch_size - 1) // batch_size  # Ceiling division\n",
    "        batch_pbar = tqdm(\n",
    "            range(0, total_rows, batch_size), \n",
    "            desc=f\"Writing batches for {os.path.basename(fpath)}\", \n",
    "            unit=\"batch\",\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "        for i in batch_pbar:\n",
    "            batch_df = df.iloc[i:i + batch_size]\n",
    "            points = create_points_batch(batch_df, symbol)\n",
    "            \n",
    "            if points:\n",
    "                # Retry logic for writing batches\n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        write_api.write(bucket=bucket, org=org, record=points)\n",
    "                        rows_written += len(points)\n",
    "                        batch_pbar.set_postfix({\"rows_written\": rows_written})\n",
    "                        break\n",
    "                    except Exception as write_error:\n",
    "                        logger.warning(f\"Write attempt {attempt + 1} failed: {write_error}\")\n",
    "                        if attempt == max_retries - 1:\n",
    "                            logger.error(f\"Failed to write batch after {max_retries} attempts\")\n",
    "                            raise\n",
    "                        time.sleep(1)  # Brief pause before retry\n",
    "        \n",
    "        batch_pbar.close()\n",
    "        \n",
    "        logger.info(f\"Successfully imported {rows_written} rows from {fpath}\")\n",
    "        return rows_written\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {fpath}\")\n",
    "        return 0\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(f\"Empty or invalid CSV file: {fpath}\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {fpath}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate required environment variables.\"\"\"\n",
    "    required_vars = [\"INFLUXDB_ORG\", \"INFLUXDB_TOKEN\", \"INFLUXDB_URL\"]\n",
    "    missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "    \n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"Missing required environment variables: {missing_vars}\")\n",
    "\n",
    "def test_influxdb_connection():\n",
    "    \"\"\"Test InfluxDB connection before processing.\"\"\"\n",
    "    try:\n",
    "        # Test connection with a simple query\n",
    "        query_api = client.query_api()\n",
    "        # Simple ping query\n",
    "        result = query_api.query(f'buckets() |> filter(fn: (r) => r.name == \"{bucket}\") |> limit(n: 1)')\n",
    "        logger.info(\"InfluxDB connection test successful\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"InfluxDB connection test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        # Validate environment\n",
    "        validate_environment()\n",
    "        \n",
    "        # Test InfluxDB connection\n",
    "        if not test_influxdb_connection():\n",
    "            logger.error(\"Cannot proceed without valid InfluxDB connection\")\n",
    "            return\n",
    "        \n",
    "        data_dir = \"/Users/orentapiero/DATA/binance_klines\"\n",
    "        \n",
    "        if not os.path.exists(data_dir):\n",
    "            logger.error(f\"Data directory does not exist: {data_dir}\")\n",
    "            return\n",
    "        \n",
    "        total_files_processed = 0\n",
    "        total_rows_imported = 0\n",
    "        \n",
    "        # Get list of symbol directories\n",
    "        symbol_dirs = [d for d in os.listdir(data_dir) \n",
    "                       if os.path.isdir(os.path.join(data_dir, d))]\n",
    "        \n",
    "        if not symbol_dirs:\n",
    "            logger.error(f\"No symbol directories found in {data_dir}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(symbol_dirs)} symbol directories to process\")\n",
    "        \n",
    "        for symbol in tqdm(symbol_dirs, desc=\"Processing symbols\"):\n",
    "            symbol_dir = os.path.join(data_dir, symbol)\n",
    "            \n",
    "            # Get CSV files in symbol directory\n",
    "            csv_files = [f for f in os.listdir(symbol_dir) if f.endswith(\".csv\")]\n",
    "            \n",
    "            if not csv_files:\n",
    "                logger.warning(f\"No CSV files found in {symbol_dir}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Processing {len(csv_files)} files for symbol {symbol}\")\n",
    "            \n",
    "            for fname in tqdm(csv_files, desc=f\"Files in {symbol}\", leave=False):\n",
    "                fpath = os.path.join(symbol_dir, fname)\n",
    "                rows_imported = process_csv_file(fpath, symbol)\n",
    "                total_rows_imported += rows_imported\n",
    "                total_files_processed += 1\n",
    "        \n",
    "        logger.info(f\"Import completed! Processed {total_files_processed} files, \"\n",
    "                   f\"imported {total_rows_imported} total rows\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Import interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during import: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            client.close()\n",
    "            logger.info(\"InfluxDB client closed\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error closing client: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe96f7d-6d96-43f0-b1dc-593160829e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT OHLCV Data:\n",
      "==================================================\n",
      "Total records: 4648\n",
      "Time range: 2025-07-28 18:32:00+00:00 to 2025-07-31 23:59:00+00:00\n",
      "\n",
      "First 5 rows:\n",
      "    result  table                      time     close      high       low  \\\n",
      "0  _result      0 2025-07-28 18:32:00+00:00  117464.7  117465.4  117440.0   \n",
      "1  _result      0 2025-07-28 18:33:00+00:00  117439.8  117464.8  117400.0   \n",
      "2  _result      0 2025-07-28 18:34:00+00:00  117423.8  117467.5  117388.0   \n",
      "3  _result      0 2025-07-28 18:35:00+00:00  117462.1  117500.0  117423.9   \n",
      "4  _result      0 2025-07-28 18:36:00+00:00  117520.0  117540.0  117462.2   \n",
      "\n",
      "       open   volume  \n",
      "0  117464.7  200.595  \n",
      "1  117464.8  267.729  \n",
      "2  117438.5  152.327  \n",
      "3  117423.9   69.893  \n",
      "4  117462.2   55.829  \n",
      "\n",
      "Last 5 rows:\n",
      "       result  table                      time     close      high       low  \\\n",
      "4643  _result      0 2025-07-31 23:55:00+00:00  115665.0  115665.0  115500.0   \n",
      "4644  _result      0 2025-07-31 23:56:00+00:00  115675.8  115768.7  115588.6   \n",
      "4645  _result      0 2025-07-31 23:57:00+00:00  115712.3  115712.4  115662.0   \n",
      "4646  _result      0 2025-07-31 23:58:00+00:00  115668.2  115735.1  115668.1   \n",
      "4647  _result      0 2025-07-31 23:59:00+00:00  115697.3  115712.7  115668.1   \n",
      "\n",
      "          open   volume  \n",
      "4643  115508.9  176.983  \n",
      "4644  115665.5  226.285  \n",
      "4645  115675.7   73.324  \n",
      "4646  115712.4   77.440  \n",
      "4647  115668.2   55.664  \n",
      "\n",
      "Price Statistics:\n",
      "Highest price: $119,258.70\n",
      "Lowest price: $115,373.90\n",
      "Latest close: $115,697.30\n",
      "Total volume: 393,011.72\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from influxdb_client import InfluxDBClient\n",
    "\n",
    "# InfluxDB configuration\n",
    "bucket = \"crypto\"\n",
    "org = os.environ[\"INFLUXDB_ORG\"]\n",
    "token = os.environ[\"INFLUXDB_TOKEN\"]\n",
    "url = os.environ[\"INFLUXDB_URL\"]\n",
    "\n",
    "def get_btc_ohlcv(days_back=7):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data for BTCUSDT from InfluxDB\n",
    "    \n",
    "    Args:\n",
    "        days_back (int): Number of days to look back (default: 7)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with columns [time, open, high, low, close, volume]\n",
    "    \"\"\"\n",
    "    \n",
    "    client = InfluxDBClient(url=url, token=token, org=org)\n",
    "    query_api = client.query_api()\n",
    "    \n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "    |> range(start: -{days_back}d)\n",
    "    |> filter(fn: (r) => r._measurement == \"klines\")\n",
    "    |> filter(fn: (r) => r.symbol == \"BTCUSDT\")\n",
    "    |> filter(fn: (r) => r._field == \"open\" or r._field == \"high\" or r._field == \"low\" or r._field == \"close\" or r._field == \"volume\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "    |> sort(columns: [\"_time\"])\n",
    "    |> keep(columns: [\"_time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        result = query_api.query_data_frame(query)\n",
    "        \n",
    "        if not result.empty:\n",
    "            # Rename _time to time for cleaner output\n",
    "            result = result.rename(columns={'_time': 'time'})\n",
    "            # Convert time to datetime if it isn't already\n",
    "            result['time'] = pd.to_datetime(result['time'])\n",
    "            # Reset index for cleaner display\n",
    "            result = result.reset_index(drop=True)\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "# Usage examples\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Get last 7 days of BTCUSDT data\n",
    "    btc_data = get_btc_ohlcv(days_back=7)\n",
    "    \n",
    "    if not btc_data.empty:\n",
    "        print(\"BTCUSDT OHLCV Data:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total records: {len(btc_data)}\")\n",
    "        print(f\"Time range: {btc_data['time'].min()} to {btc_data['time'].max()}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(btc_data.head())\n",
    "        print(\"\\nLast 5 rows:\")\n",
    "        print(btc_data.tail())\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(\"\\nPrice Statistics:\")\n",
    "        print(f\"Highest price: ${btc_data['high'].max():,.2f}\")\n",
    "        print(f\"Lowest price: ${btc_data['low'].min():,.2f}\")\n",
    "        print(f\"Latest close: ${btc_data['close'].iloc[-1]:,.2f}\")\n",
    "        print(f\"Total volume: {btc_data['volume'].sum():,.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No data found for BTCUSDT\")\n",
    "\n",
    "# Alternative: Get specific number of latest records\n",
    "def get_btc_latest(limit=100):\n",
    "    \"\"\"Get the latest N records for BTCUSDT\"\"\"\n",
    "    \n",
    "    client = InfluxDBClient(url=url, token=token, org=org)\n",
    "    query_api = client.query_api()\n",
    "    \n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "    |> range(start: -30d)\n",
    "    |> filter(fn: (r) => r._measurement == \"klines\")\n",
    "    |> filter(fn: (r) => r.symbol == \"BTCUSDT\")\n",
    "    |> filter(fn: (r) => r._field == \"open\" or r._field == \"high\" or r._field == \"low\" or r._field == \"close\" or r._field == \"volume\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "    |> sort(columns: [\"_time\"], desc: true)\n",
    "    |> limit(n: {limit})\n",
    "    |> sort(columns: [\"_time\"])\n",
    "    |> keep(columns: [\"_time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        result = query_api.query_data_frame(query)\n",
    "        if not result.empty:\n",
    "            result = result.rename(columns={'_time': 'time'})\n",
    "            result['time'] = pd.to_datetime(result['time'])\n",
    "            result = result.reset_index(drop=True)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "# Quick one-liner usage:\n",
    "# btc_data = get_btc_ohlcv(days_back=1)  # Last 24 hours\n",
    "# btc_latest = get_btc_latest(limit=50)   # Latest 50 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916c6f6-922e-40df-b003-5858ee81b5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
