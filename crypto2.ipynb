{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe1e4a9-ce54-49de-acfd-056c98f821e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orentapiero/projects/Crypto/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from tqdm.notebook import tqdm\n",
    "from config import column_names,schedule\n",
    "from influxdb_client import InfluxDBClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db50fb4-f7a3-4528-b20c-72ab294ab83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfluxDB configuration\n",
    "bucket = \"crypto\"\n",
    "org = os.environ[\"INFLUXDB_ORG\"]\n",
    "token = os.environ[\"INFLUXDB_TOKEN\"]\n",
    "url = os.environ[\"INFLUXDB_URL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80a2346-f8c3-4bcc-abdb-8a0432d8d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ohlcv_aggregated(symbol=\"BTCUSDT\", start_date=None, end_date=None, interval=\"1h\"):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data with proper OHLCV aggregation from InfluxDB within a date range\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Trading pair symbol (default: \"BTCUSDT\")\n",
    "        start_date (str or datetime): Start date (default: 7 days ago)\n",
    "        end_date (str or datetime): End date (default: now)\n",
    "        interval (str): Time interval for aggregation (default: \"1h\")\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with timestamp index and columns [open, high, low, close, volume]\n",
    "    \"\"\"\n",
    "    \n",
    "    client = InfluxDBClient(url=url, token=token, org=org)\n",
    "    query_api = client.query_api()\n",
    "    \n",
    "    # Handle date formatting\n",
    "    def format_date(date_input):\n",
    "        if date_input is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(date_input, str):\n",
    "            if len(date_input) == 10 and date_input.count('-') == 2:\n",
    "                date_input += \"T00:00:00Z\"\n",
    "            elif not date_input.endswith('Z'):\n",
    "                date_input += \"Z\"\n",
    "            return date_input\n",
    "        elif isinstance(date_input, datetime):\n",
    "            return date_input.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported date format: {date_input}\")\n",
    "    \n",
    "    # Set default dates if not provided\n",
    "    if start_date is None:\n",
    "        start_str = \"-7d\"\n",
    "    else:\n",
    "        start_str = format_date(start_date)\n",
    "    \n",
    "    if end_date is None:\n",
    "        end_str = \"now()\"\n",
    "    else:\n",
    "        end_str = format_date(end_date)\n",
    "    \n",
    "    # Build the time range part of the query\n",
    "    if start_date is None:\n",
    "        range_clause = f'|> range(start: {start_str})'\n",
    "    else:\n",
    "        if end_date is None:\n",
    "            range_clause = f'|> range(start: {start_str})'\n",
    "        else:\n",
    "            range_clause = f'|> range(start: {start_str}, stop: {end_str})'\n",
    "    \n",
    "    query = f'''\n",
    "    // Get all base data first\n",
    "    base_data = from(bucket: \"{bucket}\")\n",
    "    {range_clause}\n",
    "    |> filter(fn: (r) => r._measurement == \"klines\")\n",
    "    |> filter(fn: (r) => r.symbol == \"{symbol}\")\n",
    "    |> filter(fn: (r) => r._field == \"open\" or r._field == \"high\" or r._field == \"low\" or r._field == \"close\" or r._field == \"volume\")\n",
    "    \n",
    "    // Aggregate OPEN: first value in each time window\n",
    "    open_data = base_data\n",
    "    |> filter(fn: (r) => r._field == \"open\")\n",
    "    |> aggregateWindow(every: {interval}, fn: first, createEmpty: false)\n",
    "    |> set(key: \"_field\", value: \"open\")\n",
    "    \n",
    "    // Aggregate HIGH: maximum value in each time window\n",
    "    high_data = base_data\n",
    "    |> filter(fn: (r) => r._field == \"high\")\n",
    "    |> aggregateWindow(every: {interval}, fn: max, createEmpty: false)\n",
    "    |> set(key: \"_field\", value: \"high\")\n",
    "    \n",
    "    // Aggregate LOW: minimum value in each time window  \n",
    "    low_data = base_data\n",
    "    |> filter(fn: (r) => r._field == \"low\")\n",
    "    |> aggregateWindow(every: {interval}, fn: min, createEmpty: false)\n",
    "    |> set(key: \"_field\", value: \"low\")\n",
    "    \n",
    "    // Aggregate CLOSE: last value in each time window\n",
    "    close_data = base_data\n",
    "    |> filter(fn: (r) => r._field == \"close\")\n",
    "    |> aggregateWindow(every: {interval}, fn: last, createEmpty: false)\n",
    "    |> set(key: \"_field\", value: \"close\")\n",
    "    \n",
    "    // Aggregate VOLUME: sum all volumes in each time window\n",
    "    volume_data = base_data\n",
    "    |> filter(fn: (r) => r._field == \"volume\")\n",
    "    |> aggregateWindow(every: {interval}, fn: sum, createEmpty: false)\n",
    "    |> set(key: \"_field\", value: \"volume\")\n",
    "    \n",
    "    // Union all aggregated data and pivot\n",
    "    union(tables: [open_data, high_data, low_data, close_data, volume_data])\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "    |> sort(columns: [\"_time\"])\n",
    "    |> keep(columns: [\"_time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        result = query_api.query_data_frame(query)\n",
    "        \n",
    "        if not result.empty:\n",
    "            # Convert time to datetime\n",
    "            result['_time'] = pd.to_datetime(result['_time'])\n",
    "            \n",
    "            # Ensure numeric columns are properly typed\n",
    "            numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "            for col in numeric_columns:\n",
    "                if col in result.columns:\n",
    "                    result[col] = pd.to_numeric(result[col], errors='coerce')\n",
    "            \n",
    "            # Set timestamp as index\n",
    "            result = result.set_index('_time')\n",
    "            result = result.sort_index()\n",
    "            result.index.name = 'timestamp'\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching aggregated data for {symbol} from {start_date} to {end_date}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b095d504-001d-4300-b8a7-9d060054dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tickers_ohlcv(start_date=None, end_date=None, interval=\"1h\", max_workers=5):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data for all available tickers from InfluxDB within a date range\n",
    "    \n",
    "    Args:\n",
    "        start_date (str or datetime): Start date (default: 7 days ago)\n",
    "        end_date (str or datetime): End date (default: now)\n",
    "        interval (str): Time interval for aggregation (default: \"1h\")\n",
    "        max_workers (int): Maximum number of concurrent threads (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with ticker symbols as keys and DataFrames as values\n",
    "              Format: {'BTCUSDT': DataFrame, 'ETHUSDT': DataFrame, ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_available_tickers(start_date, end_date):\n",
    "        \"\"\"Get all unique tickers available in the specified date range\"\"\"\n",
    "        client = InfluxDBClient(url=url, token=token, org=org)\n",
    "        query_api = client.query_api()\n",
    "        \n",
    "        # Handle date formatting\n",
    "        def format_date(date_input):\n",
    "            if date_input is None:\n",
    "                return None\n",
    "            \n",
    "            if isinstance(date_input, str):\n",
    "                if len(date_input) == 10 and date_input.count('-') == 2:\n",
    "                    date_input += \"T00:00:00Z\"\n",
    "                elif not date_input.endswith('Z'):\n",
    "                    date_input += \"Z\"\n",
    "                return date_input\n",
    "            elif isinstance(date_input, datetime):\n",
    "                return date_input.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported date format: {date_input}\")\n",
    "        \n",
    "        # Set default dates if not provided\n",
    "        if start_date is None:\n",
    "            start_str = \"-7d\"\n",
    "        else:\n",
    "            start_str = format_date(start_date)\n",
    "        \n",
    "        if end_date is None:\n",
    "            end_str = \"now()\"\n",
    "        else:\n",
    "            end_str = format_date(end_date)\n",
    "        \n",
    "        # Build the time range part of the query\n",
    "        if start_date is None:\n",
    "            range_clause = f'|> range(start: {start_str})'\n",
    "        else:\n",
    "            if end_date is None:\n",
    "                range_clause = f'|> range(start: {start_str})'\n",
    "            else:\n",
    "                range_clause = f'|> range(start: {start_str}, stop: {end_str})'\n",
    "        \n",
    "        # Query to get all unique tickers\n",
    "        ticker_query = f'''\n",
    "        from(bucket: \"{bucket}\")\n",
    "        {range_clause}\n",
    "        |> filter(fn: (r) => r._measurement == \"klines\")\n",
    "        |> keep(columns: [\"symbol\"])\n",
    "        |> distinct(column: \"symbol\")\n",
    "        |> group()\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            result = query_api.query_data_frame(ticker_query)\n",
    "            if not result.empty and 'symbol' in result.columns:\n",
    "                tickers = result['symbol'].unique().tolist()\n",
    "                print(f\"Found {len(tickers)} tickers in the specified date range\")\n",
    "                return tickers\n",
    "            else:\n",
    "                print(\"No tickers found in the specified date range\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching tickers: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            client.close()\n",
    "    \n",
    "    def fetch_single_ticker_ohlcv(symbol):\n",
    "        \"\"\"Fetch OHLCV for a single ticker\"\"\"\n",
    "        try:\n",
    "            df = get_ohlcv_aggregated(symbol, start_date, end_date, interval)\n",
    "            if not df.empty:\n",
    "                return symbol, df\n",
    "            else:\n",
    "                print(f\"No data found for {symbol}\")\n",
    "                return symbol, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return symbol, None\n",
    "    \n",
    "    # Get all available tickers\n",
    "    print(\"Fetching available tickers...\")\n",
    "    tickers = get_available_tickers(start_date, end_date)\n",
    "    \n",
    "    if not tickers:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Fetching OHLCV data for {len(tickers)} tickers...\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    ohlcv_data = {}\n",
    "    \n",
    "    # Fetch data for all tickers using threading for better performance\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_symbol = {\n",
    "            executor.submit(fetch_single_ticker_ohlcv, symbol): symbol \n",
    "            for symbol in tickers\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol, data = future.result()\n",
    "            if data is not None:\n",
    "                ohlcv_data[symbol] = data\n",
    "            \n",
    "            completed += 1\n",
    "            if completed % 10 == 0 or completed == len(tickers):\n",
    "                print(f\"Completed: {completed}/{len(tickers)} tickers\")\n",
    "    \n",
    "    print(f\"Successfully fetched data for {len(ohlcv_data)} out of {len(tickers)} tickers\")\n",
    "    return ohlcv_data\n",
    "\n",
    "\n",
    "def get_all_tickers_ohlcv_sequential(start_date=None, end_date=None, interval=\"1h\", delay=0.1):\n",
    "    \"\"\"\n",
    "    Sequential version - use this if you want to avoid overwhelming the database\n",
    "    \n",
    "    Args:\n",
    "        start_date (str or datetime): Start date (default: 7 days ago)\n",
    "        end_date (str or datetime): End date (default: now)\n",
    "        interval (str): Time interval for aggregation (default: \"1h\")\n",
    "        delay (float): Delay between requests in seconds (default: 0.1)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with ticker symbols as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_available_tickers(start_date, end_date):\n",
    "        \"\"\"Get all unique tickers available in the specified date range\"\"\"\n",
    "        client = InfluxDBClient(url=url, token=token, org=org)\n",
    "        query_api = client.query_api()\n",
    "        \n",
    "        # Handle date formatting (same as above)\n",
    "        def format_date(date_input):\n",
    "            if date_input is None:\n",
    "                return None\n",
    "            \n",
    "            if isinstance(date_input, str):\n",
    "                if len(date_input) == 10 and date_input.count('-') == 2:\n",
    "                    date_input += \"T00:00:00Z\"\n",
    "                elif not date_input.endswith('Z'):\n",
    "                    date_input += \"Z\"\n",
    "                return date_input\n",
    "            elif isinstance(date_input, datetime):\n",
    "                return date_input.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported date format: {date_input}\")\n",
    "        \n",
    "        if start_date is None:\n",
    "            start_str = \"-7d\"\n",
    "        else:\n",
    "            start_str = format_date(start_date)\n",
    "        \n",
    "        if end_date is None:\n",
    "            end_str = \"now()\"\n",
    "        else:\n",
    "            end_str = format_date(end_date)\n",
    "        \n",
    "        if start_date is None:\n",
    "            range_clause = f'|> range(start: {start_str})'\n",
    "        else:\n",
    "            if end_date is None:\n",
    "                range_clause = f'|> range(start: {start_str})'\n",
    "            else:\n",
    "                range_clause = f'|> range(start: {start_str}, stop: {end_str})'\n",
    "        \n",
    "        ticker_query = f'''\n",
    "        from(bucket: \"{bucket}\")\n",
    "        {range_clause}\n",
    "        |> filter(fn: (r) => r._measurement == \"klines\")\n",
    "        |> keep(columns: [\"symbol\"])\n",
    "        |> distinct(column: \"symbol\")\n",
    "        |> group()\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            result = query_api.query_data_frame(ticker_query)\n",
    "            if not result.empty and 'symbol' in result.columns:\n",
    "                tickers = result['symbol'].unique().tolist()\n",
    "                print(f\"Found {len(tickers)} tickers in the specified date range\")\n",
    "                return tickers\n",
    "            else:\n",
    "                print(\"No tickers found in the specified date range\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching tickers: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            client.close()\n",
    "    \n",
    "    # Get all available tickers\n",
    "    print(\"Fetching available tickers...\")\n",
    "    tickers = get_available_tickers(start_date, end_date)\n",
    "    \n",
    "    if not tickers:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Fetching OHLCV data for {len(tickers)} tickers sequentially...\")\n",
    "    \n",
    "    ohlcv_data = {}\n",
    "    \n",
    "    for i, symbol in enumerate(tickers, 1):\n",
    "        try:\n",
    "            df = get_ohlcv_aggregated(symbol, start_date, end_date, interval)\n",
    "            if not df.empty:\n",
    "                ohlcv_data[symbol] = df\n",
    "            else:\n",
    "                print(f\"No data found for {symbol}\")\n",
    "            \n",
    "            if i % 10 == 0 or i == len(tickers):\n",
    "                print(f\"Completed: {i}/{len(tickers)} tickers\")\n",
    "            \n",
    "            # Add delay to avoid overwhelming the database\n",
    "            if delay > 0 and i < len(tickers):\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully fetched data for {len(ohlcv_data)} out of {len(tickers)} tickers\")\n",
    "    return ohlcv_data\n",
    "\n",
    "\n",
    "def save_all_tickers_to_files(ohlcv_data, output_dir=\"ohlcv_data\", file_format=\"csv\"):\n",
    "    \"\"\"\n",
    "    Save all ticker data to separate files\n",
    "    \n",
    "    Args:\n",
    "        ohlcv_data (dict): Dictionary from get_all_tickers_ohlcv()\n",
    "        output_dir (str): Directory to save files\n",
    "        file_format (str): File format - \"csv\", \"parquet\", or \"pickle\"\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for symbol, df in ohlcv_data.items():\n",
    "        if file_format.lower() == \"csv\":\n",
    "            filename = f\"{output_dir}/{symbol}_ohlcv.csv\"\n",
    "            df.to_csv(filename)\n",
    "        elif file_format.lower() == \"parquet\":\n",
    "            filename = f\"{output_dir}/{symbol}_ohlcv.parquet\"\n",
    "            df.to_parquet(filename)\n",
    "        elif file_format.lower() == \"pickle\":\n",
    "            filename = f\"{output_dir}/{symbol}_ohlcv.pkl\"\n",
    "            df.to_pickle(filename)\n",
    "        else:\n",
    "            raise ValueError(\"file_format must be 'csv', 'parquet', or 'pickle'\")\n",
    "    \n",
    "    print(f\"Saved {len(ohlcv_data)} files to {output_dir}/\")\n",
    "\n",
    "def get_all_tickers_ohlcv_with_progress(start_date=None, end_date=None, interval=\"1h\", max_workers=5):\n",
    "    \"\"\"\n",
    "    Enhanced version with better progress tracking\n",
    "    \n",
    "    Args:\n",
    "        start_date (str or datetime): Start date (default: 7 days ago)\n",
    "        end_date (str or datetime): End date (default: now)\n",
    "        interval (str): Time interval for aggregation (default: \"1h\")\n",
    "        max_workers (int): Maximum number of concurrent threads (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with ticker symbols as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_available_tickers(start_date, end_date):\n",
    "        \"\"\"Get all unique tickers available in the specified date range\"\"\"\n",
    "        client = InfluxDBClient(url=url, token=token, org=org)\n",
    "        query_api = client.query_api()\n",
    "        \n",
    "        # Handle date formatting\n",
    "        def format_date(date_input):\n",
    "            if date_input is None:\n",
    "                return None\n",
    "            \n",
    "            if isinstance(date_input, str):\n",
    "                if len(date_input) == 10 and date_input.count('-') == 2:\n",
    "                    date_input += \"T00:00:00Z\"\n",
    "                elif not date_input.endswith('Z'):\n",
    "                    date_input += \"Z\"\n",
    "                return date_input\n",
    "            elif isinstance(date_input, datetime):\n",
    "                return date_input.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported date format: {date_input}\")\n",
    "        \n",
    "        # Set default dates if not provided\n",
    "        if start_date is None:\n",
    "            start_str = \"-7d\"\n",
    "        else:\n",
    "            start_str = format_date(start_date)\n",
    "        \n",
    "        if end_date is None:\n",
    "            end_str = \"now()\"\n",
    "        else:\n",
    "            end_str = format_date(end_date)\n",
    "        \n",
    "        # Build the time range part of the query\n",
    "        if start_date is None:\n",
    "            range_clause = f'|> range(start: {start_str})'\n",
    "        else:\n",
    "            if end_date is None:\n",
    "                range_clause = f'|> range(start: {start_str})'\n",
    "            else:\n",
    "                range_clause = f'|> range(start: {start_str}, stop: {end_str})'\n",
    "        \n",
    "        # Query to get all unique tickers\n",
    "        ticker_query = f'''\n",
    "        from(bucket: \"{bucket}\")\n",
    "        {range_clause}\n",
    "        |> filter(fn: (r) => r._measurement == \"klines\")\n",
    "        |> keep(columns: [\"symbol\"])\n",
    "        |> distinct(column: \"symbol\")\n",
    "        |> group()\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            result = query_api.query_data_frame(ticker_query)\n",
    "            if not result.empty and 'symbol' in result.columns:\n",
    "                tickers = result['symbol'].unique().tolist()\n",
    "                print(f\"Found {len(tickers)} tickers in the specified date range\")\n",
    "                return tickers\n",
    "            else:\n",
    "                print(\"No tickers found in the specified date range\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching tickers: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            client.close()\n",
    "    \n",
    "    def fetch_single_ticker_ohlcv(symbol):\n",
    "        \"\"\"Fetch OHLCV for a single ticker\"\"\"\n",
    "        try:\n",
    "            df = get_ohlcv_aggregated(symbol, start_date, end_date, interval)\n",
    "            if not df.empty:\n",
    "                return symbol, df\n",
    "            else:\n",
    "                return symbol, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return symbol, None\n",
    "    \n",
    "    # Get all available tickers\n",
    "    print(\"Fetching available tickers...\")\n",
    "    tickers = get_available_tickers(start_date, end_date)\n",
    "    \n",
    "    if not tickers:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Fetching OHLCV data for {len(tickers)} tickers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ohlcv_data = {}\n",
    "    failed_tickers = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_symbol = {\n",
    "            executor.submit(fetch_single_ticker_ohlcv, symbol): symbol \n",
    "            for symbol in tickers\n",
    "        }\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol, data = future.result()\n",
    "            if data is not None:\n",
    "                ohlcv_data[symbol] = data\n",
    "                print(f\"✓ {symbol}: {len(data)} records\")\n",
    "            else:\n",
    "                failed_tickers.append(symbol)\n",
    "                print(f\"✗ {symbol}: No data\")\n",
    "            \n",
    "            completed += 1\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / completed\n",
    "            eta = avg_time * (len(tickers) - completed)\n",
    "            \n",
    "            print(f\"Progress: {completed}/{len(tickers)} ({completed/len(tickers)*100:.1f}%) \"\n",
    "                  f\"- ETA: {eta:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCompleted in {total_time:.1f}s\")\n",
    "    print(f\"Success: {len(ohlcv_data)}, Failed: {len(failed_tickers)}\")\n",
    "    if failed_tickers:\n",
    "        print(f\"Failed tickers: {failed_tickers[:10]}...\" if len(failed_tickers) > 10 else f\"Failed tickers: {failed_tickers}\")\n",
    "    \n",
    "    return ohlcv_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f64283-ace2-424e-ba38-9fba5be7188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching available tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orentapiero/projects/Crypto/venv/lib/python3.9/site-packages/influxdb_client/client/warnings.py:31: MissingPivotFunction: The query doesn't contains the pivot() function.\n",
      "\n",
      "The result will not be shaped to optimal processing by pandas.DataFrame. Use the pivot() function by:\n",
      "\n",
      "    \n",
      "        from(bucket: \"crypto\")\n",
      "        |> range(start: 2024-01-01T00:00:00Z, stop: 2024-01-31T00:00:00Z)\n",
      "        |> filter(fn: (r) => r._measurement == \"klines\")\n",
      "        |> keep(columns: [\"symbol\"])\n",
      "        |> distinct(column: \"symbol\")\n",
      "        |> group()\n",
      "         |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
      "\n",
      "You can disable this warning by:\n",
      "    import warnings\n",
      "    from influxdb_client.client.warnings import MissingPivotFunction\n",
      "\n",
      "    warnings.simplefilter(\"ignore\", MissingPivotFunction)\n",
      "\n",
      "For more info see:\n",
      "    - https://docs.influxdata.com/resources/videos/pivots-in-flux/\n",
      "    - https://docs.influxdata.com/flux/latest/stdlib/universe/pivot/\n",
      "    - https://docs.influxdata.com/flux/latest/stdlib/influxdata/influxdb/schema/fieldsascols/\n",
      "\n",
      "  warnings.warn(message, MissingPivotFunction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216 tickers in the specified date range\n",
      "Fetching OHLCV data for 216 tickers...\n",
      "Completed: 10/216 tickers\n",
      "Completed: 20/216 tickers\n",
      "Completed: 30/216 tickers\n",
      "Completed: 40/216 tickers\n",
      "Completed: 50/216 tickers\n",
      "Completed: 60/216 tickers\n",
      "Completed: 70/216 tickers\n",
      "Completed: 80/216 tickers\n",
      "Completed: 90/216 tickers\n",
      "Completed: 100/216 tickers\n",
      "Completed: 110/216 tickers\n",
      "Completed: 120/216 tickers\n",
      "Completed: 130/216 tickers\n",
      "Completed: 140/216 tickers\n",
      "Completed: 150/216 tickers\n",
      "Completed: 160/216 tickers\n",
      "Completed: 170/216 tickers\n",
      "Completed: 180/216 tickers\n",
      "Completed: 190/216 tickers\n",
      "Completed: 200/216 tickers\n",
      "Completed: 210/216 tickers\n",
      "Completed: 216/216 tickers\n",
      "Successfully fetched data for 216 out of 216 tickers\n"
     ]
    }
   ],
   "source": [
    "start_date,end_date = \"2024-01-01\",\"2024-01-31\"\n",
    "btc_data = get_ohlcv_aggregated(\"BTCUSDT\", start_date,end_date,interval=\"1m\")\n",
    "\n",
    "ohlcv_data = get_all_tickers_ohlcv(start_date=start_date, end_date=end_date, interval=\"1m\", max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb753e-d854-4073-a04d-512458cd1280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
